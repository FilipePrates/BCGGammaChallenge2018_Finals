{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import metrics\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação dos modelos preditivos que usam as variáveis booleanas da base das escolas para prever o Ideb2019. O processo é muito similar ao dos modelos numéricos, porem não iremos aplicar a PCA (tentei aplicar uma Multiple Correspondence Analysis (MCA), como visto nesse paper, https://www.utdallas.edu/~herve/Abdi-MCA2007-pretty.pdf , porem tive problemas na implementação do paper, já que não achei uma biblioteca com o MCA já implementado para python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0709 = pd.read_csv(r'C:\\Users\\Filipe Prates\\Documents\\Projects\\Datasets\\BCG Challenge\\original\\pred_2anos\\train_0709_bool.csv')\n",
    "train0911 = pd.read_csv(r'C:\\Users\\Filipe Prates\\Documents\\Projects\\Datasets\\BCG Challenge\\original\\pred_2anos\\train_0911_bool.csv')\n",
    "train1113 = pd.read_csv(r'C:\\Users\\Filipe Prates\\Documents\\Projects\\Datasets\\BCG Challenge\\original\\pred_2anos\\train_1113_bool.csv')\n",
    "train1315 = pd.read_csv(r'C:\\Users\\Filipe Prates\\Documents\\Projects\\Datasets\\BCG Challenge\\original\\pred_2anos\\train_1315_bool.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred1517 = pd.read_csv(r'C:\\Users\\Filipe Prates\\Documents\\Projects\\Datasets\\BCG Challenge\\original\\pred_2anos\\pred_1517_bool.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0709_idebs = pd.DataFrame()\n",
    "train0911_idebs = pd.DataFrame()\n",
    "train1113_idebs = pd.DataFrame()\n",
    "train1315_idebs = pd.DataFrame()\n",
    "pred1517_idebs = pd.DataFrame()\n",
    "\n",
    "train0709_idebs['Ideb2007'] = train0709['Ideb2007']\n",
    "train0709_idebs['Ideb2009'] = train0709['Ideb2009']\n",
    "\n",
    "train0911_idebs['Ideb2009'] = train0911['Ideb2009']\n",
    "train0911_idebs['Ideb2011'] = train0911['Ideb2011']\n",
    "\n",
    "train1113_idebs['Ideb2011'] = train1113['Ideb2011']\n",
    "train1113_idebs['Ideb2013'] = train1113['Ideb2013']\n",
    "\n",
    "train1315_idebs['Ideb2013'] = train1315['Ideb2013']\n",
    "train1315_idebs['Ideb2015'] = train1315['Ideb2015']\n",
    "\n",
    "pred1517_idebs['Ideb2015'] = pred1517['Ideb2015']\n",
    "pred1517_idebs['Ideb2017'] = pred1517['Ideb2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0709_target = train0709[['Ideb2011']]\n",
    "train0709.drop(columns=['Ideb2011','Ideb2013','Ideb2015','Ideb2017'],inplace=True)\n",
    "\n",
    "train0911_target = train0911[['Ideb2013']]\n",
    "train0911.drop(columns=['Ideb2007','Ideb2013','Ideb2015','Ideb2017'],inplace=True)\n",
    "\n",
    "train1113_target = train1113[['Ideb2015']]\n",
    "train1113.drop(columns=['Ideb2007','Ideb2009','Ideb2015','Ideb2017'],inplace=True)\n",
    "\n",
    "train1315_target = train1315[['Ideb2017']]\n",
    "train1315.drop(columns=['Ideb2007','Ideb2009','Ideb2011','Ideb2017'],inplace=True)\n",
    "\n",
    "pred1517.drop(columns=['Ideb2007','Ideb2009','Ideb2011','Ideb2013'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'in_comum_creche0',\n",
       " 'in_comum_pre0',\n",
       " 'in_comum_fund_ai0',\n",
       " 'in_comum_fund_af0',\n",
       " 'in_comum_medio_integrado0',\n",
       " 'in_comum_medio_normal0',\n",
       " 'in_comum_medio_medio0',\n",
       " 'in_em_atividade0',\n",
       " 'in_local_func_predio_escolar0',\n",
       " 'in_local_func_salas_empresa0',\n",
       " 'in_local_func_socioeducativo0',\n",
       " 'in_local_func_unid_prisional0',\n",
       " 'in_local_func_prisional_socio0',\n",
       " 'in_local_func_templo_igreja0',\n",
       " 'in_local_func_casa_professor0',\n",
       " 'in_local_func_salas_outra_esc0',\n",
       " 'in_predio_compartilhado0',\n",
       " 'in_agua_filtrada0',\n",
       " 'in_agua_rede_publica0',\n",
       " 'in_agua_poco_artesiano0',\n",
       " 'in_agua_cacimba0',\n",
       " 'in_agua_fonte_rio0',\n",
       " 'in_agua_inexistente0',\n",
       " 'in_energia_rede_publica0',\n",
       " 'in_energia_gerador0',\n",
       " 'in_energia_outros0',\n",
       " 'in_energia_inexistente0',\n",
       " 'in_esgoto_rede_publica0',\n",
       " 'in_esgoto_fossa0',\n",
       " 'in_esgoto_inexistente0',\n",
       " 'in_lixo_coleta_periodica0',\n",
       " 'in_lixo_queima0',\n",
       " 'in_lixo_joga_outra_area0',\n",
       " 'in_lixo_recicla0',\n",
       " 'in_lixo_enterra0',\n",
       " 'in_lixo_outros0',\n",
       " 'in_sala_diretoria0',\n",
       " 'in_sala_professor0',\n",
       " 'in_laboratorio_informatica0',\n",
       " 'in_laboratorio_ciencias0',\n",
       " 'in_sala_atendimento_especial0',\n",
       " 'in_quadra_esportes_coberta0',\n",
       " 'in_quadra_esportes_descoberta0',\n",
       " 'in_quadra_esportes0',\n",
       " 'in_cozinha0',\n",
       " 'in_biblioteca0',\n",
       " 'in_sala_leitura0',\n",
       " 'in_parque_infantil0',\n",
       " 'in_bercario0',\n",
       " 'in_banheiro_fora_predio0',\n",
       " 'in_banheiro_dentro_predio0',\n",
       " 'in_banheiro_ei0',\n",
       " 'in_dependencias_pne0',\n",
       " 'in_secretaria0',\n",
       " 'in_banheiro_chuveiro0',\n",
       " 'in_refeitorio0',\n",
       " 'in_despensa0',\n",
       " 'in_almoxarifado0',\n",
       " 'in_auditorio0',\n",
       " 'in_patio_coberto0',\n",
       " 'in_patio_descoberto0',\n",
       " 'in_alojam_aluno0',\n",
       " 'in_alojam_professor0',\n",
       " 'in_area_verde0',\n",
       " 'in_lavanderia0',\n",
       " 'in_internet0',\n",
       " 'in_banda_larga0',\n",
       " 'in_alimentacao0',\n",
       " 'in_educacao_indigena0',\n",
       " 'in_brasil_alfabetizado0',\n",
       " 'in_final_semana0',\n",
       " 'Cod_Escola_Completo',\n",
       " 'Ideb2015',\n",
       " 'Ideb2017',\n",
       " 'is_anosiniciais0',\n",
       " 'in_comum_creche0.1',\n",
       " 'in_comum_pre0.1',\n",
       " 'in_comum_fund_ai0.1',\n",
       " 'in_comum_fund_af0.1',\n",
       " 'in_comum_medio_integrado0.1',\n",
       " 'in_comum_medio_normal0.1',\n",
       " 'in_comum_medio_medio0.1',\n",
       " 'in_em_atividade0.1',\n",
       " 'in_local_func_predio_escolar0.1',\n",
       " 'in_local_func_salas_empresa0.1',\n",
       " 'in_local_func_socioeducativo0.1',\n",
       " 'in_local_func_unid_prisional0.1',\n",
       " 'in_local_func_prisional_socio0.1',\n",
       " 'in_local_func_templo_igreja0.1',\n",
       " 'in_local_func_casa_professor0.1',\n",
       " 'in_local_func_salas_outra_esc0.1',\n",
       " 'in_predio_compartilhado0.1',\n",
       " 'in_agua_filtrada0.1',\n",
       " 'in_agua_rede_publica0.1',\n",
       " 'in_agua_poco_artesiano0.1',\n",
       " 'in_agua_cacimba0.1',\n",
       " 'in_agua_fonte_rio0.1',\n",
       " 'in_agua_inexistente0.1',\n",
       " 'in_energia_rede_publica0.1',\n",
       " 'in_energia_gerador0.1',\n",
       " 'in_energia_outros0.1',\n",
       " 'in_energia_inexistente0.1',\n",
       " 'in_esgoto_rede_publica0.1',\n",
       " 'in_esgoto_fossa0.1',\n",
       " 'in_esgoto_inexistente0.1',\n",
       " 'in_lixo_coleta_periodica0.1',\n",
       " 'in_lixo_queima0.1',\n",
       " 'in_lixo_joga_outra_area0.1',\n",
       " 'in_lixo_recicla0.1',\n",
       " 'in_lixo_enterra0.1',\n",
       " 'in_lixo_outros0.1',\n",
       " 'in_sala_diretoria0.1',\n",
       " 'in_sala_professor0.1',\n",
       " 'in_laboratorio_informatica0.1',\n",
       " 'in_laboratorio_ciencias0.1',\n",
       " 'in_sala_atendimento_especial0.1',\n",
       " 'in_quadra_esportes_coberta0.1',\n",
       " 'in_quadra_esportes_descoberta0.1',\n",
       " 'in_quadra_esportes0.1',\n",
       " 'in_cozinha0.1',\n",
       " 'in_biblioteca0.1',\n",
       " 'in_sala_leitura0.1',\n",
       " 'in_parque_infantil0.1',\n",
       " 'in_bercario0.1',\n",
       " 'in_banheiro_fora_predio0.1',\n",
       " 'in_banheiro_dentro_predio0.1',\n",
       " 'in_banheiro_ei0.1',\n",
       " 'in_dependencias_pne0.1',\n",
       " 'in_secretaria0.1',\n",
       " 'in_banheiro_chuveiro0.1',\n",
       " 'in_refeitorio0.1',\n",
       " 'in_despensa0.1',\n",
       " 'in_almoxarifado0.1',\n",
       " 'in_auditorio0.1',\n",
       " 'in_patio_coberto0.1',\n",
       " 'in_patio_descoberto0.1',\n",
       " 'in_alojam_aluno0.1',\n",
       " 'in_alojam_professor0.1',\n",
       " 'in_area_verde0.1',\n",
       " 'in_lavanderia0.1',\n",
       " 'in_internet0.1',\n",
       " 'in_banda_larga0.1',\n",
       " 'in_alimentacao0.1',\n",
       " 'in_educacao_indigena0.1',\n",
       " 'in_brasil_alfabetizado0.1',\n",
       " 'in_final_semana0.1',\n",
       " 'is_anosiniciais0.1']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pred1517.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cod_Escolas = pred1517['Cod_Escola_Completo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0709.drop(columns=['Unnamed: 0','Cod_Escola_Completo','is_anosiniciais0','is_anosiniciais0.1'],inplace=True)\n",
    "train0911.drop(columns=['Unnamed: 0','Cod_Escola_Completo','is_anosiniciais','is_anosiniciais0'],inplace=True)\n",
    "train1113.drop(columns=['Unnamed: 0','Cod_Escola_Completo','is_anosiniciais','is_anosiniciais0'],inplace=True)\n",
    "train1315.drop(columns=['Unnamed: 0','Cod_Escola_Completo','is_anosiniciais','is_anosiniciais0'],inplace=True)\n",
    "\n",
    "pred1517.drop(columns=['Unnamed: 0','Cod_Escola_Completo','is_anosiniciais0','is_anosiniciais0.1'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Filipe Prates\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype bool, float64 were all converted to float64 by the scale function.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Filipe Prates\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype bool, float64 were all converted to float64 by the scale function.\n",
      "  \n",
      "C:\\Users\\Filipe Prates\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype bool, float64 were all converted to float64 by the scale function.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Filipe Prates\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype bool, float64 were all converted to float64 by the scale function.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Filipe Prates\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype bool, float64 were all converted to float64 by the scale function.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train0709_s = scale(train0709)\n",
    "train0911_s = scale(train0911)\n",
    "train1113_s = scale(train1113)\n",
    "train1315_s = scale(train1315)\n",
    "\n",
    "pred1517_s = scale(pred1517)\n",
    "\n",
    "train0709_idebs_s = scale(train0709_idebs)\n",
    "train0911_idebs_s = scale(train0911_idebs)\n",
    "train1113_idebs_s = scale(train1113_idebs)\n",
    "train1315_idebs_s = scale(train1315_idebs)\n",
    "\n",
    "pred1517_idebs_s = scale(pred1517_idebs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo treinado com 0709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0709, X_test_0709, y_train_0709, y_test_0709 = train_test_split(train0709,train0709_target,test_size=0.1,random_state=932)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0709 = Sequential()\n",
    "n_cols = X_train_0709.shape[1]\n",
    "model_0709.add(Dense(43,activation='relu',input_shape = (n_cols,)))\n",
    "model_0709.add(Dense(23,activation='relu',input_shape = (n_cols,)))\n",
    "#model_0709.add(Dense(13,activation='relu',input_shape = (n_cols,)))\n",
    "model_0709.add(Dense(1))\n",
    "early_stopping_monitor = EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23838 samples, validate on 5960 samples\n",
      "Epoch 1/100\n",
      "23838/23838 [==============================] - 2s 82us/step - loss: 0.7863 - val_loss: 0.2855\n",
      "Epoch 2/100\n",
      "23838/23838 [==============================] - 2s 64us/step - loss: 0.2932 - val_loss: 0.2853\n",
      "Epoch 3/100\n",
      "23838/23838 [==============================] - 1s 57us/step - loss: 0.2913 - val_loss: 0.2813\n",
      "Epoch 4/100\n",
      "23838/23838 [==============================] - 1s 56us/step - loss: 0.2898 - val_loss: 0.2837\n",
      "Epoch 5/100\n",
      "23838/23838 [==============================] - 1s 56us/step - loss: 0.2883 - val_loss: 0.2810\n",
      "Epoch 6/100\n",
      "23838/23838 [==============================] - 1s 58us/step - loss: 0.2867 - val_loss: 0.2802\n",
      "Epoch 7/100\n",
      "23838/23838 [==============================] - 1s 58us/step - loss: 0.2848 - val_loss: 0.2819\n",
      "Epoch 8/100\n",
      "23838/23838 [==============================] - 1s 59us/step - loss: 0.2861 - val_loss: 0.2932\n",
      "Epoch 9/100\n",
      "23838/23838 [==============================] - 1s 57us/step - loss: 0.2849 - val_loss: 0.2771\n",
      "Epoch 10/100\n",
      "23838/23838 [==============================] - 1s 58us/step - loss: 0.2838 - val_loss: 0.2844\n",
      "Epoch 11/100\n",
      "23838/23838 [==============================] - 2s 64us/step - loss: 0.2830 - val_loss: 0.2779\n",
      "Epoch 12/100\n",
      "23838/23838 [==============================] - 2s 69us/step - loss: 0.2801 - val_loss: 0.2797\n",
      "Epoch 13/100\n",
      "23838/23838 [==============================] - 2s 66us/step - loss: 0.2809 - val_loss: 0.2849\n",
      "Epoch 14/100\n",
      "23838/23838 [==============================] - 1s 59us/step - loss: 0.2803 - val_loss: 0.2811\n",
      "Epoch 15/100\n",
      "23838/23838 [==============================] - 1s 60us/step - loss: 0.2810 - val_loss: 0.2770\n",
      "Epoch 16/100\n",
      "23838/23838 [==============================] - 2s 65us/step - loss: 0.2795 - val_loss: 0.2835\n",
      "Epoch 17/100\n",
      "23838/23838 [==============================] - 1s 63us/step - loss: 0.2769 - val_loss: 0.2779\n",
      "Epoch 18/100\n",
      "23838/23838 [==============================] - 1s 58us/step - loss: 0.2779 - val_loss: 0.3032\n",
      "Epoch 19/100\n",
      "23838/23838 [==============================] - 1s 59us/step - loss: 0.2756 - val_loss: 0.2825\n",
      "Epoch 20/100\n",
      "23838/23838 [==============================] - 1s 60us/step - loss: 0.2764 - val_loss: 0.2830\n",
      "Epoch 21/100\n",
      "23838/23838 [==============================] - 1s 59us/step - loss: 0.2759 - val_loss: 0.2781\n",
      "Epoch 22/100\n",
      "23838/23838 [==============================] - 2s 68us/step - loss: 0.2751 - val_loss: 0.2878\n",
      "Epoch 23/100\n",
      "23838/23838 [==============================] - 2s 68us/step - loss: 0.2730 - val_loss: 0.2798\n",
      "Epoch 24/100\n",
      "23838/23838 [==============================] - 2s 65us/step - loss: 0.2731 - val_loss: 0.2819\n",
      "Epoch 25/100\n",
      "23838/23838 [==============================] - 1s 60us/step - loss: 0.2724 - val_loss: 0.2874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea2ebe6b00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0709.compile(optimizer='adam',loss='mean_squared_error')\n",
    "model_0709.fit(X_train_0709,y_train_0709,validation_split = 0.2,epochs=100,callbacks=[early_stopping_monitor],batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3311/3311 [==============================] - 0s 26us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2893818975197012"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0709.evaluate(X_test_0709,y_test_0709)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0709.save(r'C:\\Users\\Filipe Prates\\Documents\\Projects\\Datasets\\BCG Challenge\\original\\models_keras\\model_0709_bool.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo treinado com 0911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0911, X_test_0911, y_train_0911, y_test_0911 = train_test_split(train0911,train0911_target,test_size=0.1,random_state=932)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0911 = Sequential()\n",
    "n_cols = X_train_0911.shape[1]\n",
    "model_0911.add(Dense(25,activation='relu',input_shape = (n_cols,)))\n",
    "model_0911.add(Dense(13,activation='relu',input_shape = (n_cols,)))\n",
    "#model_0911.add(Dense(13,activation='relu',input_shape = (n_cols,)))\n",
    "model_0911.add(Dense(1))\n",
    "early_stopping_monitor = EarlyStopping(patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23838 samples, validate on 5960 samples\n",
      "Epoch 1/100\n",
      "23838/23838 [==============================] - 2s 90us/step - loss: 0.6093 - val_loss: 0.3126\n",
      "Epoch 2/100\n",
      "23838/23838 [==============================] - 2s 71us/step - loss: 0.2961 - val_loss: 0.3056\n",
      "Epoch 3/100\n",
      "23838/23838 [==============================] - 2s 65us/step - loss: 0.2963 - val_loss: 0.3069\n",
      "Epoch 4/100\n",
      "23838/23838 [==============================] - 1s 59us/step - loss: 0.2966 - val_loss: 0.3081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea52602f60>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0911.compile(optimizer='adam',loss='mean_squared_error')\n",
    "model_0911.fit(X_train_0911,y_train_0911,validation_split = 0.2,epochs=100,callbacks=[early_stopping_monitor],batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3311/3311 [==============================] - 0s 30us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3027386831997817"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0911.evaluate(X_test_0911,y_test_0911)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0911.save(r'C:\\Users\\Filipe Prates\\Documents\\Projects\\Datasets\\BCG Challenge\\original\\models_keras\\model_0911_bool.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo treinado com 1113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1113, X_test_1113, y_train_1113, y_test_1113 = train_test_split(train1113,train1113_target,test_size=0.1,random_state=932)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1113 = Sequential()\n",
    "n_cols = X_train_1113.shape[1]\n",
    "model_1113.add(Dense(43,activation='relu',input_shape = (n_cols,)))\n",
    "model_1113.add(Dense(20,activation='relu',input_shape = (n_cols,)))\n",
    "#model_1113.add(Dense(13,activation='relu',input_shape = (n_cols,)))\n",
    "model_1113.add(Dense(1))\n",
    "early_stopping_monitor = EarlyStopping(patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23838 samples, validate on 5960 samples\n",
      "Epoch 1/100\n",
      "23838/23838 [==============================] - 2s 86us/step - loss: 0.5246 - val_loss: 0.2866\n",
      "Epoch 2/100\n",
      "23838/23838 [==============================] - 1s 61us/step - loss: 0.2773 - val_loss: 0.2767\n",
      "Epoch 3/100\n",
      "23838/23838 [==============================] - 2s 68us/step - loss: 0.2745 - val_loss: 0.2833\n",
      "Epoch 4/100\n",
      "23838/23838 [==============================] - 2s 74us/step - loss: 0.2760 - val_loss: 0.2735\n",
      "Epoch 5/100\n",
      "23838/23838 [==============================] - 2s 69us/step - loss: 0.2717 - val_loss: 0.2713\n",
      "Epoch 6/100\n",
      "23838/23838 [==============================] - 1s 63us/step - loss: 0.2723 - val_loss: 0.3262\n",
      "Epoch 7/100\n",
      "23838/23838 [==============================] - 2s 69us/step - loss: 0.2733 - val_loss: 0.2827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea56189c18>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1113.compile(optimizer='adam',loss='mean_squared_error')\n",
    "model_1113.fit(X_train_1113,y_train_1113,validation_split = 0.2,epochs=100,callbacks=[early_stopping_monitor],batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3311/3311 [==============================] - 0s 32us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2762785462572005"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1113.evaluate(X_test_1113,y_test_1113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1113.save(r'C:\\Users\\Filipe Prates\\Documents\\Projects\\Datasets\\BCG Challenge\\original\\models_keras\\model_1113_bool.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo treinado com 1315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1315, X_test_1315, y_train_1315, y_test_1315 = train_test_split(train1315,train1315_target,test_size=0.1,random_state=932)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1315 = Sequential()\n",
    "n_cols = X_train_1315.shape[1]\n",
    "model_1315.add(Dense(38,activation='relu',input_shape = (n_cols,)))\n",
    "model_1315.add(Dense(23,activation='relu',input_shape = (n_cols,)))\n",
    "#model_1315.add(Dense(13,activation='relu',input_shape = (n_cols,)))\n",
    "model_1315.add(Dense(1))\n",
    "early_stopping_monitor = EarlyStopping(patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23838 samples, validate on 5960 samples\n",
      "Epoch 1/100\n",
      "23838/23838 [==============================] - 2s 93us/step - loss: 0.5272 - val_loss: 0.2612\n",
      "Epoch 2/100\n",
      "23838/23838 [==============================] - 1s 63us/step - loss: 0.2660 - val_loss: 0.3006\n",
      "Epoch 3/100\n",
      "23838/23838 [==============================] - 2s 64us/step - loss: 0.2622 - val_loss: 0.2550\n",
      "Epoch 4/100\n",
      "23838/23838 [==============================] - 2s 64us/step - loss: 0.2626 - val_loss: 0.3154\n",
      "Epoch 5/100\n",
      "23838/23838 [==============================] - 2s 65us/step - loss: 0.2604 - val_loss: 0.2516\n",
      "Epoch 6/100\n",
      "23838/23838 [==============================] - 2s 65us/step - loss: 0.2591 - val_loss: 0.2559\n",
      "Epoch 7/100\n",
      "23838/23838 [==============================] - 2s 64us/step - loss: 0.2584 - val_loss: 0.2515\n",
      "Epoch 8/100\n",
      "23838/23838 [==============================] - 2s 65us/step - loss: 0.2578 - val_loss: 0.2572\n",
      "Epoch 9/100\n",
      "23838/23838 [==============================] - 2s 64us/step - loss: 0.2554 - val_loss: 0.2512\n",
      "Epoch 10/100\n",
      "23838/23838 [==============================] - 2s 74us/step - loss: 0.2549 - val_loss: 0.2532\n",
      "Epoch 11/100\n",
      "23838/23838 [==============================] - 2s 75us/step - loss: 0.2527 - val_loss: 0.2517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea49636e80>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1315.compile(optimizer='adam',loss='mean_squared_error')\n",
    "model_1315.fit(X_train_1315,y_train_1315,validation_split = 0.2,epochs=100,callbacks=[early_stopping_monitor],batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3311/3311 [==============================] - 0s 30us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2644954842856484"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1315.evaluate(X_test_1315,y_test_1315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1315.save(r'C:\\Users\\Filipe Prates\\Documents\\Projects\\Datasets\\BCG Challenge\\original\\models_keras\\model_1315_bool.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pred1517).to_csv(r'C:\\Users\\Filipe Prates\\Documents\\Projects\\Datasets\\BCG Challenge\\original\\dados_predicao\\pred1517_bool.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vendo resultado dos modelos na base 1517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_0709 = pd.Series(data=model_0709.predict(pred1517)[:,0])\n",
    "pred_0911 = pd.Series(data=model_0911.predict(pred1517)[:,0])\n",
    "pred_1113 = pd.Series(data=model_1113.predict(pred1517)[:,0])\n",
    "pred_1315 = pd.Series(data=model_1315.predict(pred1517)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideb2011_media = 4.679378 #train0709_target.mean()\n",
    "ideb2013_media = 4.827755 #train0911_target.mean()\n",
    "ideb2015_media = 5.131154 #train1113_target.mean()\n",
    "ideb2017_media = 5.352877 #train1315_target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cte = (5.352877/5.131154 + 5.131154/4.827755 + 4.827755/4.679378)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame()\n",
    "output['pred_0709_bool'] = pred_0709#(pred1517['Ideb2017'].mean() + (pred_0709 - ideb2011_media))\n",
    "output['pred_0911_bool'] = pred_0911#(pred1517['Ideb2017'].mean() + (pred_0911 - ideb2013_media))\n",
    "output['pred_1113_bool'] = pred_1113#(pred1517['Ideb2017'].mean() + (pred_1113 - ideb2015_media))\n",
    "output['pred_1315_bool'] = pred_1315#(pred1517['Ideb2017'].mean() + (pred_1315 - ideb2017_media))\n",
    "output['ensemble_bool'] = ((output['pred_0709_bool']+output['pred_0911_bool']+output['pred_1113_bool']+output['pred_1315_bool'])/4)\n",
    "output['Ideb2017'] = pred1517['Ideb2017'] \n",
    "output['dif'] = output['ensemble_bool'] - output['Ideb2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ea5bde3a58>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output['dif'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_0709_bool</th>\n",
       "      <th>pred_0911_bool</th>\n",
       "      <th>pred_1113_bool</th>\n",
       "      <th>pred_1315_bool</th>\n",
       "      <th>ensemble_bool</th>\n",
       "      <th>Ideb2017</th>\n",
       "      <th>dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>33109.000000</td>\n",
       "      <td>33109.000000</td>\n",
       "      <td>33109.000000</td>\n",
       "      <td>33109.000000</td>\n",
       "      <td>33109.000000</td>\n",
       "      <td>33109.000000</td>\n",
       "      <td>33109.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.686888</td>\n",
       "      <td>5.412178</td>\n",
       "      <td>5.510764</td>\n",
       "      <td>5.450119</td>\n",
       "      <td>5.514972</td>\n",
       "      <td>5.352974</td>\n",
       "      <td>0.162010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.916164</td>\n",
       "      <td>0.970378</td>\n",
       "      <td>1.016308</td>\n",
       "      <td>1.040072</td>\n",
       "      <td>0.964227</td>\n",
       "      <td>1.101314</td>\n",
       "      <td>0.277640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.549642</td>\n",
       "      <td>2.154695</td>\n",
       "      <td>2.508620</td>\n",
       "      <td>2.352255</td>\n",
       "      <td>2.474143</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>-1.366984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.021593</td>\n",
       "      <td>4.700875</td>\n",
       "      <td>4.741983</td>\n",
       "      <td>4.654648</td>\n",
       "      <td>4.796675</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>-0.019069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.669899</td>\n",
       "      <td>5.391757</td>\n",
       "      <td>5.460251</td>\n",
       "      <td>5.401623</td>\n",
       "      <td>5.473536</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>0.159827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.350487</td>\n",
       "      <td>6.133194</td>\n",
       "      <td>6.283236</td>\n",
       "      <td>6.248546</td>\n",
       "      <td>6.246613</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>0.339480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.533525</td>\n",
       "      <td>9.096869</td>\n",
       "      <td>9.648744</td>\n",
       "      <td>9.266863</td>\n",
       "      <td>9.386499</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>1.890576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred_0709_bool  pred_0911_bool  pred_1113_bool  pred_1315_bool  \\\n",
       "count    33109.000000    33109.000000    33109.000000    33109.000000   \n",
       "mean         5.686888        5.412178        5.510764        5.450119   \n",
       "std          0.916164        0.970378        1.016308        1.040072   \n",
       "min          2.549642        2.154695        2.508620        2.352255   \n",
       "25%          5.021593        4.700875        4.741983        4.654648   \n",
       "50%          5.669899        5.391757        5.460251        5.401623   \n",
       "75%          6.350487        6.133194        6.283236        6.248546   \n",
       "max          9.533525        9.096869        9.648744        9.266863   \n",
       "\n",
       "       ensemble_bool      Ideb2017           dif  \n",
       "count   33109.000000  33109.000000  33109.000000  \n",
       "mean        5.514972      5.352974      0.162010  \n",
       "std         0.964227      1.101314      0.277640  \n",
       "min         2.474143      1.600000     -1.366984  \n",
       "25%         4.796675      4.600000     -0.019069  \n",
       "50%         5.473536      5.300000      0.159827  \n",
       "75%         6.246613      6.200000      0.339480  \n",
       "max         9.386499      9.600000      1.890576  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_0709</th>\n",
       "      <th>pred_0911</th>\n",
       "      <th>pred_1113</th>\n",
       "      <th>pred_1315</th>\n",
       "      <th>ensemble</th>\n",
       "      <th>Ideb2017</th>\n",
       "      <th>dif</th>\n",
       "      <th>Cod_Escola</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16639</th>\n",
       "      <td>8.916245</td>\n",
       "      <td>7.826588</td>\n",
       "      <td>7.712863</td>\n",
       "      <td>8.716056</td>\n",
       "      <td>8.292938</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2.767938</td>\n",
       "      <td>35216185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30107</th>\n",
       "      <td>8.022028</td>\n",
       "      <td>7.424187</td>\n",
       "      <td>7.503023</td>\n",
       "      <td>7.696084</td>\n",
       "      <td>7.661330</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.761330</td>\n",
       "      <td>41129636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28522</th>\n",
       "      <td>7.809846</td>\n",
       "      <td>7.334523</td>\n",
       "      <td>7.357326</td>\n",
       "      <td>7.524714</td>\n",
       "      <td>7.506603</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.756603</td>\n",
       "      <td>35227407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29896</th>\n",
       "      <td>8.392632</td>\n",
       "      <td>7.751066</td>\n",
       "      <td>7.539706</td>\n",
       "      <td>8.495644</td>\n",
       "      <td>8.044762</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.669762</td>\n",
       "      <td>41093755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29955</th>\n",
       "      <td>8.826130</td>\n",
       "      <td>7.548514</td>\n",
       "      <td>7.758229</td>\n",
       "      <td>8.237893</td>\n",
       "      <td>8.092692</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.667692</td>\n",
       "      <td>41368940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17041</th>\n",
       "      <td>9.598798</td>\n",
       "      <td>7.993013</td>\n",
       "      <td>8.231568</td>\n",
       "      <td>8.555511</td>\n",
       "      <td>8.594722</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.644722</td>\n",
       "      <td>42116031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16646</th>\n",
       "      <td>8.536139</td>\n",
       "      <td>7.530887</td>\n",
       "      <td>7.665279</td>\n",
       "      <td>8.004235</td>\n",
       "      <td>7.934135</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2.634135</td>\n",
       "      <td>35090136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28529</th>\n",
       "      <td>7.563078</td>\n",
       "      <td>6.955121</td>\n",
       "      <td>7.391695</td>\n",
       "      <td>7.508910</td>\n",
       "      <td>7.354701</td>\n",
       "      <td>7.2</td>\n",
       "      <td>2.629701</td>\n",
       "      <td>35245948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30010</th>\n",
       "      <td>7.899292</td>\n",
       "      <td>7.437760</td>\n",
       "      <td>6.954862</td>\n",
       "      <td>7.861715</td>\n",
       "      <td>7.538407</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.613407</td>\n",
       "      <td>41133331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>8.938924</td>\n",
       "      <td>7.831657</td>\n",
       "      <td>7.825237</td>\n",
       "      <td>8.111258</td>\n",
       "      <td>8.176769</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.601769</td>\n",
       "      <td>35020102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30097</th>\n",
       "      <td>8.249425</td>\n",
       "      <td>7.145858</td>\n",
       "      <td>7.462104</td>\n",
       "      <td>7.828431</td>\n",
       "      <td>7.671454</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.596454</td>\n",
       "      <td>41026667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28552</th>\n",
       "      <td>8.202469</td>\n",
       "      <td>7.116069</td>\n",
       "      <td>7.108586</td>\n",
       "      <td>7.746062</td>\n",
       "      <td>7.543297</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.593297</td>\n",
       "      <td>35050647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29891</th>\n",
       "      <td>7.397295</td>\n",
       "      <td>6.731203</td>\n",
       "      <td>6.764882</td>\n",
       "      <td>7.376108</td>\n",
       "      <td>7.067372</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.592372</td>\n",
       "      <td>41130260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16651</th>\n",
       "      <td>8.063305</td>\n",
       "      <td>7.196952</td>\n",
       "      <td>7.661915</td>\n",
       "      <td>7.746819</td>\n",
       "      <td>7.667248</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.592248</td>\n",
       "      <td>35281694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19622</th>\n",
       "      <td>9.168462</td>\n",
       "      <td>7.571900</td>\n",
       "      <td>7.952803</td>\n",
       "      <td>8.350805</td>\n",
       "      <td>8.260992</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2.585992</td>\n",
       "      <td>42019940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14222</th>\n",
       "      <td>9.768113</td>\n",
       "      <td>8.565995</td>\n",
       "      <td>8.790172</td>\n",
       "      <td>9.169265</td>\n",
       "      <td>9.073386</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.573386</td>\n",
       "      <td>23219289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>8.060165</td>\n",
       "      <td>6.926828</td>\n",
       "      <td>7.408195</td>\n",
       "      <td>7.640987</td>\n",
       "      <td>7.509044</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.559044</td>\n",
       "      <td>35902548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28476</th>\n",
       "      <td>7.963364</td>\n",
       "      <td>6.839533</td>\n",
       "      <td>7.334952</td>\n",
       "      <td>7.665005</td>\n",
       "      <td>7.450713</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.550713</td>\n",
       "      <td>35274159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30026</th>\n",
       "      <td>7.392291</td>\n",
       "      <td>6.781637</td>\n",
       "      <td>6.926815</td>\n",
       "      <td>7.212788</td>\n",
       "      <td>7.078383</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.528383</td>\n",
       "      <td>41131193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30093</th>\n",
       "      <td>8.181284</td>\n",
       "      <td>7.256443</td>\n",
       "      <td>7.554118</td>\n",
       "      <td>7.920470</td>\n",
       "      <td>7.728079</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.528079</td>\n",
       "      <td>41024117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14016</th>\n",
       "      <td>9.132112</td>\n",
       "      <td>7.633185</td>\n",
       "      <td>7.957684</td>\n",
       "      <td>8.674051</td>\n",
       "      <td>8.349258</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.524258</td>\n",
       "      <td>23025379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22861</th>\n",
       "      <td>8.877624</td>\n",
       "      <td>7.229180</td>\n",
       "      <td>8.149342</td>\n",
       "      <td>8.338105</td>\n",
       "      <td>8.148563</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2.523563</td>\n",
       "      <td>35024636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>8.222061</td>\n",
       "      <td>6.973510</td>\n",
       "      <td>7.433416</td>\n",
       "      <td>7.664507</td>\n",
       "      <td>7.573373</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.523373</td>\n",
       "      <td>31075345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16908</th>\n",
       "      <td>8.640609</td>\n",
       "      <td>7.209002</td>\n",
       "      <td>7.385642</td>\n",
       "      <td>7.740393</td>\n",
       "      <td>7.743911</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.518911</td>\n",
       "      <td>42125421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28562</th>\n",
       "      <td>8.706413</td>\n",
       "      <td>7.471880</td>\n",
       "      <td>7.773399</td>\n",
       "      <td>8.213872</td>\n",
       "      <td>8.041391</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.516391</td>\n",
       "      <td>35242081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28792</th>\n",
       "      <td>8.485075</td>\n",
       "      <td>7.438907</td>\n",
       "      <td>7.781469</td>\n",
       "      <td>8.035577</td>\n",
       "      <td>7.935257</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.510257</td>\n",
       "      <td>35207263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29111</th>\n",
       "      <td>9.027994</td>\n",
       "      <td>7.561059</td>\n",
       "      <td>7.978129</td>\n",
       "      <td>8.467902</td>\n",
       "      <td>8.258771</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.508771</td>\n",
       "      <td>35240801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30034</th>\n",
       "      <td>8.666565</td>\n",
       "      <td>7.811007</td>\n",
       "      <td>8.039391</td>\n",
       "      <td>8.709734</td>\n",
       "      <td>8.306674</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2.506674</td>\n",
       "      <td>41063732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29885</th>\n",
       "      <td>7.070850</td>\n",
       "      <td>6.181851</td>\n",
       "      <td>6.527545</td>\n",
       "      <td>6.838555</td>\n",
       "      <td>6.654700</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.504700</td>\n",
       "      <td>41134613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19614</th>\n",
       "      <td>8.423013</td>\n",
       "      <td>7.447256</td>\n",
       "      <td>7.508575</td>\n",
       "      <td>7.795416</td>\n",
       "      <td>7.793565</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2.493565</td>\n",
       "      <td>42091616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25826</th>\n",
       "      <td>4.246495</td>\n",
       "      <td>3.151452</td>\n",
       "      <td>3.560533</td>\n",
       "      <td>3.858521</td>\n",
       "      <td>3.704250</td>\n",
       "      <td>2.9</td>\n",
       "      <td>-0.995750</td>\n",
       "      <td>29153484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6945</th>\n",
       "      <td>4.730572</td>\n",
       "      <td>3.691662</td>\n",
       "      <td>4.017832</td>\n",
       "      <td>4.060155</td>\n",
       "      <td>4.125055</td>\n",
       "      <td>3.9</td>\n",
       "      <td>-0.999945</td>\n",
       "      <td>25072307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>4.082673</td>\n",
       "      <td>3.098538</td>\n",
       "      <td>3.140621</td>\n",
       "      <td>3.272944</td>\n",
       "      <td>3.398694</td>\n",
       "      <td>2.6</td>\n",
       "      <td>-1.001306</td>\n",
       "      <td>29222710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24616</th>\n",
       "      <td>4.189102</td>\n",
       "      <td>3.645707</td>\n",
       "      <td>3.677614</td>\n",
       "      <td>3.877007</td>\n",
       "      <td>3.847357</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-1.002643</td>\n",
       "      <td>15098079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24080</th>\n",
       "      <td>4.822603</td>\n",
       "      <td>3.904443</td>\n",
       "      <td>3.849299</td>\n",
       "      <td>4.504267</td>\n",
       "      <td>4.270153</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-1.004847</td>\n",
       "      <td>15130223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21621</th>\n",
       "      <td>4.494058</td>\n",
       "      <td>3.713112</td>\n",
       "      <td>3.579283</td>\n",
       "      <td>3.575373</td>\n",
       "      <td>3.840456</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-1.009544</td>\n",
       "      <td>25114417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21745</th>\n",
       "      <td>3.701718</td>\n",
       "      <td>2.799076</td>\n",
       "      <td>2.872275</td>\n",
       "      <td>2.782818</td>\n",
       "      <td>3.038972</td>\n",
       "      <td>2.7</td>\n",
       "      <td>-1.011028</td>\n",
       "      <td>29093937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6074</th>\n",
       "      <td>3.648841</td>\n",
       "      <td>3.351229</td>\n",
       "      <td>3.151693</td>\n",
       "      <td>3.240262</td>\n",
       "      <td>3.348006</td>\n",
       "      <td>2.8</td>\n",
       "      <td>-1.026994</td>\n",
       "      <td>29290791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>5.003908</td>\n",
       "      <td>4.240226</td>\n",
       "      <td>4.348747</td>\n",
       "      <td>4.298587</td>\n",
       "      <td>4.472867</td>\n",
       "      <td>4.1</td>\n",
       "      <td>-1.027133</td>\n",
       "      <td>22030174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21066</th>\n",
       "      <td>4.229574</td>\n",
       "      <td>2.699323</td>\n",
       "      <td>2.740316</td>\n",
       "      <td>3.096672</td>\n",
       "      <td>3.191471</td>\n",
       "      <td>2.8</td>\n",
       "      <td>-1.033529</td>\n",
       "      <td>29229189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7120</th>\n",
       "      <td>4.117292</td>\n",
       "      <td>3.473761</td>\n",
       "      <td>3.676234</td>\n",
       "      <td>3.632725</td>\n",
       "      <td>3.725003</td>\n",
       "      <td>2.4</td>\n",
       "      <td>-1.049997</td>\n",
       "      <td>29404991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>3.949067</td>\n",
       "      <td>3.375395</td>\n",
       "      <td>3.129352</td>\n",
       "      <td>3.207053</td>\n",
       "      <td>3.415216</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>29153913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6243</th>\n",
       "      <td>4.299543</td>\n",
       "      <td>3.743006</td>\n",
       "      <td>3.753233</td>\n",
       "      <td>3.764412</td>\n",
       "      <td>3.890049</td>\n",
       "      <td>2.9</td>\n",
       "      <td>-1.059951</td>\n",
       "      <td>31220663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5691</th>\n",
       "      <td>4.186751</td>\n",
       "      <td>2.884011</td>\n",
       "      <td>3.541884</td>\n",
       "      <td>3.510990</td>\n",
       "      <td>3.530909</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-1.069091</td>\n",
       "      <td>16004884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25508</th>\n",
       "      <td>4.830566</td>\n",
       "      <td>4.455240</td>\n",
       "      <td>4.541182</td>\n",
       "      <td>4.659527</td>\n",
       "      <td>4.621629</td>\n",
       "      <td>4.1</td>\n",
       "      <td>-1.078371</td>\n",
       "      <td>27033503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6749</th>\n",
       "      <td>4.455398</td>\n",
       "      <td>3.180243</td>\n",
       "      <td>3.508966</td>\n",
       "      <td>3.223490</td>\n",
       "      <td>3.592024</td>\n",
       "      <td>2.8</td>\n",
       "      <td>-1.082976</td>\n",
       "      <td>15034690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24821</th>\n",
       "      <td>4.843876</td>\n",
       "      <td>3.598235</td>\n",
       "      <td>3.940758</td>\n",
       "      <td>3.763336</td>\n",
       "      <td>4.036551</td>\n",
       "      <td>2.7</td>\n",
       "      <td>-1.088449</td>\n",
       "      <td>21004560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21618</th>\n",
       "      <td>3.887418</td>\n",
       "      <td>2.948228</td>\n",
       "      <td>3.330425</td>\n",
       "      <td>2.838062</td>\n",
       "      <td>3.251033</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-1.098967</td>\n",
       "      <td>25118927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6752</th>\n",
       "      <td>4.623545</td>\n",
       "      <td>3.561429</td>\n",
       "      <td>4.044684</td>\n",
       "      <td>4.029444</td>\n",
       "      <td>4.064776</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.110224</td>\n",
       "      <td>15034674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24521</th>\n",
       "      <td>4.550212</td>\n",
       "      <td>3.473546</td>\n",
       "      <td>3.761692</td>\n",
       "      <td>4.384501</td>\n",
       "      <td>4.042488</td>\n",
       "      <td>3.4</td>\n",
       "      <td>-1.132512</td>\n",
       "      <td>13072170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10643</th>\n",
       "      <td>5.101602</td>\n",
       "      <td>4.417425</td>\n",
       "      <td>4.148166</td>\n",
       "      <td>4.291877</td>\n",
       "      <td>4.489767</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.135233</td>\n",
       "      <td>52031349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21725</th>\n",
       "      <td>4.713876</td>\n",
       "      <td>3.219405</td>\n",
       "      <td>3.706255</td>\n",
       "      <td>3.639169</td>\n",
       "      <td>3.819676</td>\n",
       "      <td>2.8</td>\n",
       "      <td>-1.180324</td>\n",
       "      <td>29394112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6762</th>\n",
       "      <td>3.905428</td>\n",
       "      <td>3.129793</td>\n",
       "      <td>3.036781</td>\n",
       "      <td>3.044065</td>\n",
       "      <td>3.279017</td>\n",
       "      <td>2.3</td>\n",
       "      <td>-1.195983</td>\n",
       "      <td>15042944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25566</th>\n",
       "      <td>4.060546</td>\n",
       "      <td>3.381574</td>\n",
       "      <td>3.523039</td>\n",
       "      <td>3.986429</td>\n",
       "      <td>3.737897</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-1.212103</td>\n",
       "      <td>28016858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5761</th>\n",
       "      <td>4.437672</td>\n",
       "      <td>3.776969</td>\n",
       "      <td>4.199261</td>\n",
       "      <td>3.928970</td>\n",
       "      <td>4.085718</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-1.239282</td>\n",
       "      <td>22015752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6080</th>\n",
       "      <td>3.703050</td>\n",
       "      <td>3.400991</td>\n",
       "      <td>3.316600</td>\n",
       "      <td>3.213451</td>\n",
       "      <td>3.408523</td>\n",
       "      <td>2.6</td>\n",
       "      <td>-1.241477</td>\n",
       "      <td>29404371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18799</th>\n",
       "      <td>5.632816</td>\n",
       "      <td>3.698884</td>\n",
       "      <td>4.317013</td>\n",
       "      <td>4.776118</td>\n",
       "      <td>4.606208</td>\n",
       "      <td>4.6</td>\n",
       "      <td>-1.268792</td>\n",
       "      <td>33062110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24503</th>\n",
       "      <td>4.697889</td>\n",
       "      <td>3.835362</td>\n",
       "      <td>4.512329</td>\n",
       "      <td>4.594261</td>\n",
       "      <td>4.409960</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-1.315040</td>\n",
       "      <td>13070355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21625</th>\n",
       "      <td>3.717805</td>\n",
       "      <td>2.857507</td>\n",
       "      <td>3.119790</td>\n",
       "      <td>2.953678</td>\n",
       "      <td>3.162195</td>\n",
       "      <td>2.3</td>\n",
       "      <td>-1.337805</td>\n",
       "      <td>25122150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21764</th>\n",
       "      <td>4.405047</td>\n",
       "      <td>3.233276</td>\n",
       "      <td>3.322734</td>\n",
       "      <td>3.101324</td>\n",
       "      <td>3.515595</td>\n",
       "      <td>2.8</td>\n",
       "      <td>-1.459405</td>\n",
       "      <td>29393787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33109 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred_0709  pred_0911  pred_1113  pred_1315  ensemble  Ideb2017  \\\n",
       "16639   8.916245   7.826588   7.712863   8.716056  8.292938       8.1   \n",
       "30107   8.022028   7.424187   7.503023   7.696084  7.661330       7.3   \n",
       "28522   7.809846   7.334523   7.357326   7.524714  7.506603       7.4   \n",
       "29896   8.392632   7.751066   7.539706   8.495644  8.044762       8.0   \n",
       "29955   8.826130   7.548514   7.758229   8.237893  8.092692       7.9   \n",
       "17041   9.598798   7.993013   8.231568   8.555511  8.594722       8.6   \n",
       "16646   8.536139   7.530887   7.665279   8.004235  7.934135       8.1   \n",
       "28529   7.563078   6.955121   7.391695   7.508910  7.354701       7.2   \n",
       "30010   7.899292   7.437760   6.954862   7.861715  7.538407       7.3   \n",
       "1849    8.938924   7.831657   7.825237   8.111258  8.176769       7.9   \n",
       "30097   8.249425   7.145858   7.462104   7.828431  7.671454       7.5   \n",
       "28552   8.202469   7.116069   7.108586   7.746062  7.543297       7.3   \n",
       "29891   7.397295   6.731203   6.764882   7.376108  7.067372       6.8   \n",
       "16651   8.063305   7.196952   7.661915   7.746819  7.667248       7.5   \n",
       "19622   9.168462   7.571900   7.952803   8.350805  8.260992       8.3   \n",
       "14222   9.768113   8.565995   8.790172   9.169265  9.073386       9.5   \n",
       "1928    8.060165   6.926828   7.408195   7.640987  7.509044       7.4   \n",
       "28476   7.963364   6.839533   7.334952   7.665005  7.450713       7.4   \n",
       "30026   7.392291   6.781637   6.926815   7.212788  7.078383       6.5   \n",
       "30093   8.181284   7.256443   7.554118   7.920470  7.728079       7.5   \n",
       "14016   9.132112   7.633185   7.957684   8.674051  8.349258       9.0   \n",
       "22861   8.877624   7.229180   8.149342   8.338105  8.148563       8.2   \n",
       "1152    8.222061   6.973510   7.433416   7.664507  7.573373       7.4   \n",
       "16908   8.640609   7.209002   7.385642   7.740393  7.743911       7.5   \n",
       "28562   8.706413   7.471880   7.773399   8.213872  8.041391       8.0   \n",
       "28792   8.485075   7.438907   7.781469   8.035577  7.935257       7.5   \n",
       "29111   9.027994   7.561059   7.978129   8.467902  8.258771       8.0   \n",
       "30034   8.666565   7.811007   8.039391   8.709734  8.306674       8.4   \n",
       "29885   7.070850   6.181851   6.527545   6.838555  6.654700       6.4   \n",
       "19614   8.423013   7.447256   7.508575   7.795416  7.793565       7.7   \n",
       "...          ...        ...        ...        ...       ...       ...   \n",
       "25826   4.246495   3.151452   3.560533   3.858521  3.704250       2.9   \n",
       "6945    4.730572   3.691662   4.017832   4.060155  4.125055       3.9   \n",
       "6108    4.082673   3.098538   3.140621   3.272944  3.398694       2.6   \n",
       "24616   4.189102   3.645707   3.677614   3.877007  3.847357       3.5   \n",
       "24080   4.822603   3.904443   3.849299   4.504267  4.270153       3.6   \n",
       "21621   4.494058   3.713112   3.579283   3.575373  3.840456       3.2   \n",
       "21745   3.701718   2.799076   2.872275   2.782818  3.038972       2.7   \n",
       "6074    3.648841   3.351229   3.151693   3.240262  3.348006       2.8   \n",
       "5765    5.003908   4.240226   4.348747   4.298587  4.472867       4.1   \n",
       "21066   4.229574   2.699323   2.740316   3.096672  3.191471       2.8   \n",
       "7120    4.117292   3.473761   3.676234   3.632725  3.725003       2.4   \n",
       "6042    3.949067   3.375395   3.129352   3.207053  3.415216       3.0   \n",
       "6243    4.299543   3.743006   3.753233   3.764412  3.890049       2.9   \n",
       "5691    4.186751   2.884011   3.541884   3.510990  3.530909       3.1   \n",
       "25508   4.830566   4.455240   4.541182   4.659527  4.621629       4.1   \n",
       "6749    4.455398   3.180243   3.508966   3.223490  3.592024       2.8   \n",
       "24821   4.843876   3.598235   3.940758   3.763336  4.036551       2.7   \n",
       "21618   3.887418   2.948228   3.330425   2.838062  3.251033       2.5   \n",
       "6752    4.623545   3.561429   4.044684   4.029444  4.064776       3.0   \n",
       "24521   4.550212   3.473546   3.761692   4.384501  4.042488       3.4   \n",
       "10643   5.101602   4.417425   4.148166   4.291877  4.489767       4.0   \n",
       "21725   4.713876   3.219405   3.706255   3.639169  3.819676       2.8   \n",
       "6762    3.905428   3.129793   3.036781   3.044065  3.279017       2.3   \n",
       "25566   4.060546   3.381574   3.523039   3.986429  3.737897       3.2   \n",
       "5761    4.437672   3.776969   4.199261   3.928970  4.085718       3.6   \n",
       "6080    3.703050   3.400991   3.316600   3.213451  3.408523       2.6   \n",
       "18799   5.632816   3.698884   4.317013   4.776118  4.606208       4.6   \n",
       "24503   4.697889   3.835362   4.512329   4.594261  4.409960       3.6   \n",
       "21625   3.717805   2.857507   3.119790   2.953678  3.162195       2.3   \n",
       "21764   4.405047   3.233276   3.322734   3.101324  3.515595       2.8   \n",
       "\n",
       "            dif  Cod_Escola  \n",
       "16639  2.767938    35216185  \n",
       "30107  2.761330    41129636  \n",
       "28522  2.756603    35227407  \n",
       "29896  2.669762    41093755  \n",
       "29955  2.667692    41368940  \n",
       "17041  2.644722    42116031  \n",
       "16646  2.634135    35090136  \n",
       "28529  2.629701    35245948  \n",
       "30010  2.613407    41133331  \n",
       "1849   2.601769    35020102  \n",
       "30097  2.596454    41026667  \n",
       "28552  2.593297    35050647  \n",
       "29891  2.592372    41130260  \n",
       "16651  2.592248    35281694  \n",
       "19622  2.585992    42019940  \n",
       "14222  2.573386    23219289  \n",
       "1928   2.559044    35902548  \n",
       "28476  2.550713    35274159  \n",
       "30026  2.528383    41131193  \n",
       "30093  2.528079    41024117  \n",
       "14016  2.524258    23025379  \n",
       "22861  2.523563    35024636  \n",
       "1152   2.523373    31075345  \n",
       "16908  2.518911    42125421  \n",
       "28562  2.516391    35242081  \n",
       "28792  2.510257    35207263  \n",
       "29111  2.508771    35240801  \n",
       "30034  2.506674    41063732  \n",
       "29885  2.504700    41134613  \n",
       "19614  2.493565    42091616  \n",
       "...         ...         ...  \n",
       "25826 -0.995750    29153484  \n",
       "6945  -0.999945    25072307  \n",
       "6108  -1.001306    29222710  \n",
       "24616 -1.002643    15098079  \n",
       "24080 -1.004847    15130223  \n",
       "21621 -1.009544    25114417  \n",
       "21745 -1.011028    29093937  \n",
       "6074  -1.026994    29290791  \n",
       "5765  -1.027133    22030174  \n",
       "21066 -1.033529    29229189  \n",
       "7120  -1.049997    29404991  \n",
       "6042  -1.059784    29153913  \n",
       "6243  -1.059951    31220663  \n",
       "5691  -1.069091    16004884  \n",
       "25508 -1.078371    27033503  \n",
       "6749  -1.082976    15034690  \n",
       "24821 -1.088449    21004560  \n",
       "21618 -1.098967    25118927  \n",
       "6752  -1.110224    15034674  \n",
       "24521 -1.132512    13072170  \n",
       "10643 -1.135233    52031349  \n",
       "21725 -1.180324    29394112  \n",
       "6762  -1.195983    15042944  \n",
       "25566 -1.212103    28016858  \n",
       "5761  -1.239282    22015752  \n",
       "6080  -1.241477    29404371  \n",
       "18799 -1.268792    33062110  \n",
       "24503 -1.315040    13070355  \n",
       "21625 -1.337805    25122150  \n",
       "21764 -1.459405    29393787  \n",
       "\n",
       "[33109 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sort_dif = output.sort_values(['dif'],ascending=False)\n",
    "output_sort_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_esc_pred2 = output_sort_dif.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_esc_pred2.to_csv(r'C:\\Users\\Filipe Prates\\Documents\\Projects\\Datasets\\BCG Challenge\\original\\best_schools\\best_esc_pred2(sem_ideb)_bool.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
